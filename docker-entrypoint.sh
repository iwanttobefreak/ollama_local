#!/bin/bash
set -e

echo "ðŸš€ Iniciando contenedor Ollama con entorno Python virtual..."
echo "========================================================"

# Verificar que el entorno virtual existe
if [ ! -d "/ollama-agente" ]; then
    echo "âŒ Error: Entorno virtual no encontrado en /ollama-agente"
    exit 1
fi

# Activar entorno virtual
echo "ðŸ”§ Activando entorno virtual Python..."
source /ollama-agente/bin/activate

# Verificar activaciÃ³n
if [ -z "$VIRTUAL_ENV" ]; then
    echo "âŒ Error: No se pudo activar el entorno virtual"
    exit 1
fi

echo "âœ… Entorno virtual activado: $VIRTUAL_ENV"
echo "   Python: $(which python3)"
echo "   Pip: $(which pip)"

# Verificar que las dependencias estÃ¡n instaladas
echo "ðŸ“¦ Verificando dependencias Python..."
python3 -c "import ollama, mcp; print('âœ… Dependencias OK')" || {
    echo "âŒ Error: Dependencias Python no instaladas correctamente"
    exit 1
}

# Verificar que Ollama estÃ¡ instalado
if ! command -v ollama &> /dev/null; then
    echo "âŒ Error: Ollama no estÃ¡ instalado"
    exit 1
fi

echo "ðŸ¤– Ollama instalado correctamente"
echo "========================================================"

# Iniciar Ollama serve en background
echo "ðŸŒ Iniciando Ollama serve..."
ollama serve --host 0.0.0.0 &

# Esperar a que Ollama estÃ© listo
echo "â³ Esperando que Ollama estÃ© listo..."
sleep 5

# Verificar que Ollama responde
if curl -s http://localhost:11434/api/version > /dev/null; then
    echo "âœ… Ollama serve iniciado correctamente en puerto 11434"
else
    echo "âš ï¸  Ollama serve iniciado, pero no responde aÃºn. Puede tardar unos segundos..."
fi

# Verificar e instalar modelo si es necesario
echo "ðŸ¤– Verificando modelos disponibles..."
MODELS_RESPONSE=$(curl -s http://localhost:11434/api/tags)
if echo "$MODELS_RESPONSE" | grep -q "llama3.2:1b"; then
    echo "âœ… Modelo llama3.2:1b ya estÃ¡ disponible"
else
    echo "ðŸ“¥ Modelo llama3.2:1b no encontrado. Instalando modelo bÃ¡sico..."
    echo "â³ Esto puede tardar varios minutos..."
    
    # Instalar un modelo mÃ¡s pequeÃ±o y rÃ¡pido para testing
    ollama pull llama3.2:1b &
    MODEL_PULL_PID=$!
    echo "âœ… Descarga de modelo iniciada (PID: $MODEL_PULL_PID)"
    
    # Esperar a que el modelo se descargue (timeout de 5 minutos)
    timeout 300 bash -c "while ! curl -s http://localhost:11434/api/tags | grep -q 'llama3.2:1b'; do sleep 5; done" && echo "âœ… Modelo descargado exitosamente" || echo "âš ï¸  Timeout en descarga de modelo, continuando de todos modos..."
fi

# Iniciar la API de Flask en background
echo "ðŸŒ Iniciando API Ollama Server..."
if [ -f "/app/scrics/api/api_ollama_server.py" ]; then
    echo "ðŸ“ Cambiando a directorio: /app/scrics/api"
    cd /app/scrics/api
    echo "ðŸ“„ Archivo encontrado: $(ls -la api_ollama_server.py)"
    echo "ðŸ Ejecutando: python3 api_ollama_server.py"
    echo "ðŸ”§ Entorno virtual: $VIRTUAL_ENV"
    echo "ðŸ Python path: $(which python3)"
    
    # Ejecutar en background
    python3 api_ollama_server.py &
    API_PID=$!
    echo "âœ… API Ollama Server iniciado (PID: $API_PID) en puerto 5000"
else
    echo "âš ï¸  Archivo api_ollama_server.py no encontrado en /app/scrics/api/"
    ls -la /app/scrics/api/ 2>/dev/null || echo "Directorio no existe"
fi

echo "========================================================"
echo "ðŸŽ‰ Contenedor listo!"
echo "   - Entorno virtual: $VIRTUAL_ENV"
echo "   - Ollama corriendo en: http://localhost:11434"
echo "   - API Flask en: http://localhost:5000"
echo "========================================================"

# Mantener el contenedor corriendo
wait
