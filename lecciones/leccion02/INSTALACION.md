# LecciÃ³n 2 - GuÃ­a de InstalaciÃ³n y EjecuciÃ³n

## Requisitos Previos

### 1. Python 3.8 o superior
```bash
python3 --version
```

### 2. Ollama (opcional para ejemplos completos)

**OpciÃ³n A: Docker (recomendado)**
```bash
# Verificar si Ollama estÃ¡ corriendo
docker ps | grep ollama

# Iniciar Ollama
docker start ollama

# Verificar modelos disponibles
docker exec ollama ollama list

# Descargar modelo si no estÃ¡
docker exec ollama ollama pull llama3.2:3b
```

**OpciÃ³n B: InstalaciÃ³n local**
```bash
# macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows: Descargar de https://ollama.ai/download
```

---

## InstalaciÃ³n RÃ¡pida

### MÃ©todo 1: Script AutomÃ¡tico (Recomendado)

```bash
cd lecciones/leccion02
./test_leccion02.sh
```

El script te guiarÃ¡ por todas las opciones.

### MÃ©todo 2: InstalaciÃ³n Manual

```bash
cd lecciones/leccion02

# Instalar dependencias
pip install -r requirements.txt

# O instalar individualmente
pip install mcp>=0.9.0
pip install ollama>=0.1.0
```

---

## Ejemplos Disponibles

### 1ï¸âƒ£ Ejemplo MÃ­nimo (Sin Ollama)

**PropÃ³sito:** Entender lo bÃ¡sico de MCP sin complicaciones

```bash
python3 mcp_client_minimo.py
```

**Salida esperada:**
```
ğŸ§ª PRUEBA MÃNIMA DE MCP

âœ… Herramientas: saludar
ğŸ“¨ Respuesta: Â¡Hola MarÃ­a! Bienvenido al servidor MCP.
```

**Lo que hace:**
- Inicia un servidor MCP simple
- Se conecta como cliente
- Lista las herramientas disponibles
- Ejecuta una herramienta de prueba

---

### 2ï¸âƒ£ Ejemplo Completo (Con Ollama y Temperatura)

**PropÃ³sito:** Ver MCP en acciÃ³n con un LLM real

**Requisitos:**
- Ollama corriendo (docker o local)
- Modelo llama3.2:3b descargado

```bash
python3 mcp_client_temperatura.py
```

**Ejemplo de uso:**
```
ğŸ‘¤ TÃº: Â¿QuÃ© temperatura harÃ¡ maÃ±ana en Madrid?

ğŸ¤” Pensando...
âœ… Consultando servidor MCP...
   ğŸ”§ Herramienta: obtener_temperatura
   ğŸ“ Ciudad: Madrid

ğŸ¤– Asistente: SegÃºn el pronÃ³stico para Madrid maÃ±ana:
- Temperatura: Entre 9Â°C y 19Â°C
- Clima: Nublado
- Probabilidad de lluvia: 8%
```

---

## Estructura de Archivos

```
leccion02/
â”œâ”€â”€ README.md                      # DocumentaciÃ³n principal
â”œâ”€â”€ COMPARACION.md                 # LecciÃ³n 1 vs LecciÃ³n 2
â”œâ”€â”€ INSTALACION.md                 # Esta guÃ­a
â”œâ”€â”€ requirements.txt               # Dependencias Python
â”œâ”€â”€ test_leccion02.sh             # Script de prueba interactivo
â”‚
â”œâ”€â”€ mcp_server_minimo.py          # ğŸŸ¢ Servidor MCP simple
â”œâ”€â”€ mcp_client_minimo.py          # ğŸŸ¢ Cliente para probar lo bÃ¡sico
â”‚
â”œâ”€â”€ mcp_server_temperatura.py     # ğŸ”µ Servidor con temperatura real
â””â”€â”€ mcp_client_temperatura.py     # ğŸ”µ Cliente con Ollama integrado
```

**Leyenda:**
- ğŸŸ¢ BÃ¡sico: Sin dependencias de Ollama
- ğŸ”µ Completo: Requiere Ollama corriendo

---

## Troubleshooting

### Error: "No se ha podido resolver la importaciÃ³n 'mcp'"

**SoluciÃ³n:**
```bash
pip install mcp
```

### Error: "connection refused" al ejecutar cliente

**Causa:** El servidor MCP no se iniciÃ³ correctamente

**SoluciÃ³n:** 
- El cliente inicia el servidor automÃ¡ticamente
- Verifica que los permisos sean correctos
- Ejecuta desde el directorio `leccion02`

### Error: "Model not found" con Ollama

**SoluciÃ³n:**
```bash
# Docker
docker exec ollama ollama pull llama3.2:3b

# Local
ollama pull llama3.2:3b
```

### Docker: "Container ollama not found"

**SoluciÃ³n:**
```bash
# Listar contenedores
docker ps -a | grep ollama

# Si existe pero estÃ¡ parado
docker start ollama

# Si no existe, crearlo
docker run -d --name ollama \
  -p 11434:11434 \
  -v ollama:/root/.ollama \
  ollama/ollama
```

### El script de temperatura no funciona

**Verifica:**
```bash
# Â¿Existe el script de la lecciÃ³n 1?
ls ../leccion01/script_pronostico_temperatura.py

# Prueba ejecutarlo directamente
python3 ../leccion01/script_pronostico_temperatura.py Madrid 3
```

---

## VerificaciÃ³n de la InstalaciÃ³n

### Test Completo

```bash
cd lecciones/leccion02

# 1. Verificar Python
python3 --version

# 2. Verificar dependencias
python3 -c "import mcp; print('MCP OK')"
python3 -c "import ollama; print('Ollama OK')"

# 3. Ejecutar prueba mÃ­nima
python3 mcp_client_minimo.py

# 4. Verificar Ollama (si estÃ¡ instalado)
docker exec ollama ollama list
# O en local: ollama list

# 5. Ejecutar prueba completa
python3 mcp_client_temperatura.py
```

---

## Preguntas Frecuentes

### Â¿Necesito Ollama para todo?

**No.** El ejemplo mÃ­nimo (`mcp_client_minimo.py`) funciona sin Ollama.
Solo necesitas Ollama para el ejemplo completo con IA.

### Â¿Puedo usar otros modelos de Ollama?

**SÃ­.** Edita el archivo `mcp_client_temperatura.py` y cambia:
```python
model='llama3.2:3b'  # Cambia esto
```

Modelos recomendados:
- `llama3.2:3b` - RÃ¡pido y ligero (recomendado)
- `llama3.1:8b` - MÃ¡s preciso, mÃ¡s lento
- `mistral:7b` - Alternativa buena

### Â¿Puedo ejecutar el servidor MCP en otra mÃ¡quina?

**SÃ­**, pero esta lecciÃ³n usa `stdio` (local).
Para servidores remotos necesitarÃ­as configurar MCP sobre HTTP/WebSocket
(tema de lecciones avanzadas).

### Â¿Funciona en Windows?

**SÃ­**, pero necesitas ajustar:
1. Usar `python` en vez de `python3`
2. No ejecutar `.sh` directamente (usar Git Bash o WSL)
3. Ollama: Descargar instalador de Windows

---

## Siguientes Pasos

1. âœ… Completa el ejemplo mÃ­nimo
2. âœ… Completa el ejemplo de temperatura
3. ğŸ“– Lee [COMPARACION.md](COMPARACION.md) para entender diferencias
4. ğŸ”§ Modifica los ejemplos para tus necesidades
5. ğŸš€ Crea tu propio servidor MCP

---

## Recursos Adicionales

- **DocumentaciÃ³n MCP:** https://modelcontextprotocol.io/
- **Ollama Docs:** https://ollama.ai/docs
- **Python MCP SDK:** https://github.com/modelcontextprotocol/python-sdk
- **Ejemplos oficiales:** https://github.com/modelcontextprotocol/servers

---

**Â¿Problemas? Revisa:**
1. Los errores en la terminal
2. Que estÃ©s en el directorio correcto
3. Que las dependencias estÃ©n instaladas
4. Que Ollama estÃ© corriendo (para ejemplos completos)
