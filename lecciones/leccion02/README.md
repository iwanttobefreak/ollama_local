# Lecci√≥n 2: MCP Servers (Model Context Protocol)

## üöÄ Inicio R√°pido

```bash
cd lecciones/leccion02

# Opci√≥n 1: Usar el script interactivo
./test_leccion02.sh

# Opci√≥n 2: Instalar y probar manualmente
pip install mcp ollama
python3 mcp_client_minimo.py
```

> üìñ **Gu√≠a completa de instalaci√≥n:** Ver [INSTALACION.md](INSTALACION.md)

---

## Contenido

1. [¬øQu√© es MCP?](#qu√©-es-mcp)
2. [Ejemplo M√≠nimo (sin Ollama)](#ejemplo-m√≠nimo-sin-ollama)
3. [Ejemplo Completo con Temperatura](#ejemplo-completo-con-temperatura)
4. [Instalaci√≥n](#instalaci√≥n-de-dependencias)
5. [C√≥mo Ejecutar](#c√≥mo-ejecutar)
6. [Usando Docker con Ollama](#usando-ollama-con-docker)
7. [Ventajas de MCP](#ventajas-de-mcp-sobre-tools-directas)
8. [Comparaci√≥n con Lecci√≥n 1](#comparaci√≥n-con-lecci√≥n-1)

**üìö Documentaci√≥n adicional:**
- [INSTALACION.md](INSTALACION.md) - Gu√≠a completa de instalaci√≥n y troubleshooting
- [COMPARACION.md](COMPARACION.md) - Comparaci√≥n detallada Lecci√≥n 1 vs Lecci√≥n 2
- [RESUMEN.md](RESUMEN.md) - Resumen visual y conceptos clave
- [MODELOS.md](MODELOS.md) - Comparaci√≥n de modelos Ollama (llama3.1:8b vs llama3.2:3b)
- [TROUBLESHOOTING.md](TROUBLESHOOTING.md) - Soluci√≥n de problemas comunes
- [VALIDACION_TIPOS.md](VALIDACION_TIPOS.md) - ‚ö†Ô∏è Importante: C√≥mo validar tipos de par√°metros en MCP

---

## ¬øQu√© es MCP?

MCP (Model Context Protocol) es un protocolo est√°ndar creado por Anthropic que permite a los modelos de lenguaje conectarse con **servidores externos** que proporcionan datos y herramientas de forma estandarizada.

### Diferencia con Tools (Lecci√≥n 1):

- **Tools (Lecci√≥n 1)**: El script Python ejecuta directamente comandos locales
- **MCP Servers (Lecci√≥n 2)**: Un servidor externo proporciona las herramientas y el LLM se conecta a √©l

## Arquitectura MCP

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Cliente   ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ MCP Server  ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ   Recursos   ‚îÇ
‚îÇ  (Ollama)   ‚îÇ   MCP   ‚îÇ  (Python)   ‚îÇ         ‚îÇ  (APIs, DB)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Ejemplo M√≠nimo (sin Ollama)

Antes de integrar con Ollama, veamos un ejemplo **s√∫per simple** de MCP:

### Servidor M√≠nimo: `mcp_server_minimo.py`

```python
#!/usr/bin/env python3
import asyncio
from mcp.server import Server, NotificationOptions
from mcp.server.models import InitializationOptions
import mcp.server.stdio
import mcp.types as types

server = Server("ejemplo-minimo")

@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    return [
        types.Tool(
            name="saludar",
            description="Devuelve un saludo personalizado",
            inputSchema={
                "type": "object",
                "properties": {
                    "nombre": {"type": "string", "description": "Nombre de la persona"}
                },
                "required": ["nombre"]
            }
        )
    ]

@server.call_tool()
async def handle_call_tool(name: str, arguments: dict | None) -> list[types.TextContent]:
    if name != "saludar":
        raise ValueError(f"Herramienta desconocida: {name}")
    nombre = arguments.get("nombre", "desconocido")
    return [types.TextContent(type="text", text=f"¬°Hola {nombre}! Bienvenido al servidor MCP.")]

async def main():
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(read_stream, write_stream, 
            InitializationOptions(server_name="ejemplo-minimo", server_version="1.0.0",
                capabilities=server.get_capabilities(notification_options=NotificationOptions(), 
                    experimental_capabilities={})))

if __name__ == "__main__":
    asyncio.run(main())
```

### Cliente M√≠nimo: `mcp_client_minimo.py`

```python
#!/usr/bin/env python3
import asyncio
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def main():
    print("üß™ PRUEBA M√çNIMA DE MCP\n")
    
    server_params = StdioServerParameters(command="python3", args=["mcp_server_minimo.py"])
    
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            
            tools = await session.list_tools()
            print(f"‚úÖ Herramientas: {tools.tools[0].name}")
            
            resultado = await session.call_tool("saludar", {"nombre": "Mar√≠a"})
            print(f"üì® Respuesta: {resultado.content[0].text}")

if __name__ == "__main__":
    asyncio.run(main())
```

### Ejecutar el ejemplo m√≠nimo:

```bash
cd lecciones/leccion02
pip install mcp
python3 mcp_client_minimo.py
```

Salida esperada:
```
üß™ PRUEBA M√çNIMA DE MCP

‚úÖ Herramientas: saludar
üì® Respuesta: ¬°Hola Mar√≠a! Bienvenido al servidor MCP.
```

---

## Ejemplo Completo con Temperatura

Ahora integramos MCP con Ollama para crear un asistente meteorol√≥gico inteligente.

### 1. Arquitectura

### 1. Arquitectura

```
Usuario ‚Üí Cliente Python ‚Üí Ollama (LLM) ‚ü∑ MCP Server ‚Üí Script Temperatura ‚Üí API Open-Meteo
```

El servidor MCP b√°sico necesita:
- Definir las herramientas (tools) que ofrece
- Implementar la l√≥gica de cada herramienta
- Ejecutarse y esperar conexiones

### 2. C√≥digo del Servidor: `mcp_server_temperatura.py`

Este servidor expone una herramienta para obtener temperatura de ciudades espa√±olas:

```python
#!/usr/bin/env python3
"""
Servidor MCP simple para consultar temperatura
"""
import asyncio
import subprocess
from mcp.server import Server, NotificationOptions
from mcp.server.models import InitializationOptions
import mcp.server.stdio
import mcp.types as types

# Crear el servidor MCP
server = Server("temperatura-server")

@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """Lista las herramientas disponibles"""
    return [
        types.Tool(
            name="obtener_temperatura",
            description="Obtiene el pron√≥stico de temperatura para ciudades espa√±olas",
            inputSchema={
                "type": "object",
                "properties": {
                    "ciudad": {
                        "type": "string",
                        "description": "Nombre de la ciudad espa√±ola"
                    },
                    "dias": {
                        "type": "integer",
                        "description": "N√∫mero de d√≠as de pron√≥stico (1-16)",
                        "default": 3
                    }
                },
                "required": ["ciudad"]
            }
        )
    ]

@server.call_tool()
async def handle_call_tool(
    name: str, arguments: dict | None
) -> list[types.TextContent]:
    """Ejecuta la herramienta solicitada"""
    
    if name != "obtener_temperatura":
        raise ValueError(f"Herramienta desconocida: {name}")
    
    ciudad = arguments.get("ciudad")
    dias = arguments.get("dias", 3)
    
    # Ejecutar el script de temperatura
    try:
        resultado = subprocess.run(
            ['python3', 'script_pronostico_temperatura.py', ciudad, str(dias)],
            capture_output=True,
            text=True,
            timeout=30,
            cwd="/Users/T054810/ollama_local/lecciones/leccion01"
        )
        
        if resultado.returncode == 0:
            return [types.TextContent(
                type="text",
                text=resultado.stdout
            )]
        else:
            return [types.TextContent(
                type="text",
                text=f"Error: {resultado.stderr}"
            )]
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=f"Error ejecutando script: {str(e)}"
        )]

async def main():
    """Punto de entrada del servidor"""
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="temperatura-server",
                server_version="1.0.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                )
            )
        )

if __name__ == "__main__":
    asyncio.run(main())
```

### 3. Cliente para Conectar con el Servidor MCP: `mcp_client_temperatura.py`

```python
#!/usr/bin/env python3
"""
Cliente simple que se conecta al servidor MCP de temperatura
"""
import asyncio
import ollama
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def main():
    print("="*60)
    print("üåê CLIENTE MCP - Conexi√≥n con Servidor de Temperatura")
    print("="*60)
    
    # Par√°metros del servidor MCP
    server_params = StdioServerParameters(
        command="python3",
        args=["mcp_server_temperatura.py"],
        env=None
    )
    
    # Conectar al servidor MCP
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Inicializar sesi√≥n
            await session.initialize()
            
            # Listar herramientas disponibles
            tools = await session.list_tools()
            print(f"\n‚úÖ Conectado al servidor MCP")
            print(f"üìã Herramientas disponibles: {len(tools.tools)}")
            
            for tool in tools.tools:
                print(f"   - {tool.name}: {tool.description}")
            
            print("\n" + "="*60)
            print("Escribe 'salir' para terminar\n")
            
            # Chat con el usuario
            mensajes = [
                {
                    'role': 'system',
                    'content': 'Eres un asistente meteorol√≥gico. Usa las herramientas disponibles para responder preguntas sobre el tiempo.'
                }
            ]
            
            while True:
                pregunta = input("\nüë§ T√∫: ").strip()
                
                if pregunta.lower() in ['salir', 'exit', 'quit']:
                    print("üëã ¬°Hasta luego!")
                    break
                
                if not pregunta:
                    continue
                
                mensajes.append({'role': 'user', 'content': pregunta})
                
                # Convertir tools MCP a formato Ollama
                tools_ollama = []
                for tool in tools.tools:
                    tools_ollama.append({
                        'type': 'function',
                        'function': {
                            'name': tool.name,
                            'description': tool.description,
                            'parameters': tool.inputSchema
                        }
                    })
                
                # Primera llamada: LLM decide
                print("\nü§î Pensando...")
                respuesta = ollama.chat(
                    model='llama3.1:8b',
                    messages=mensajes,
                    tools=tools_ollama
                )
                
                # ¬øEl LLM quiere usar una herramienta?
                if respuesta['message'].get('tool_calls'):
                    print("‚úÖ Usando herramienta MCP...")
                    
                    tool_call = respuesta['message']['tool_calls'][0]
                    tool_name = tool_call['function']['name']
                    tool_args = tool_call['function']['arguments']
                    
                    print(f"üîß Herramienta: {tool_name}")
                    print(f"üìù Argumentos: {tool_args}")
                    
                    # Llamar a la herramienta en el servidor MCP
                    resultado = await session.call_tool(tool_name, tool_args)
                    
                    resultado_texto = resultado.content[0].text
                    
                    # A√±adir resultado al historial
                    mensajes.append(respuesta['message'])
                    mensajes.append({
                        'role': 'tool',
                        'content': resultado_texto
                    })
                    
                    # Segunda llamada: LLM procesa el resultado
                    respuesta = ollama.chat(
                        model='llama3.1:8b',
                        messages=mensajes
                    )
                
                # Mostrar respuesta
                print(f"\nü§ñ Asistente: {respuesta['message']['content']}")
                mensajes.append(respuesta['message'])

if __name__ == "__main__":
    asyncio.run(main())
```

## Instalaci√≥n de Dependencias

Para ejecutar los ejemplos de MCP necesitas instalar el SDK de MCP:

```bash
pip install mcp
pip install ollama
```

## C√≥mo Ejecutar

### Opci√≥n 1: Ejecutar el Servidor Manualmente (para debugging)

En una terminal, ejecuta el servidor:
```bash
cd lecciones/leccion02
python3 mcp_server_temperatura.py
```

El servidor se quedar√° esperando conexiones.

### Opci√≥n 2: Ejecutar el Cliente (autom√°tico)

El cliente inicia el servidor autom√°ticamente:
```bash
cd lecciones/leccion02
python3 mcp_client_temperatura.py
```

## Usando Ollama con Docker

Si tienes Ollama en Docker (contenedor llamado "ollama"), aseg√∫rate de que est√© corriendo:

```bash
# Ver si est√° corriendo
docker ps | grep ollama

# Si no est√° corriendo, iniciarlo
docker start ollama

# Verificar que el modelo est√© disponible
docker exec ollama ollama list
```

Para ejecutar los scripts con Ollama en Docker, el cliente Python se conecta al API de Ollama (por defecto en `http://localhost:11434`).

## Ejemplo de Uso

```
üë§ T√∫: ¬øQu√© temperatura har√° ma√±ana en Madrid?

ü§î Pensando...
‚úÖ Usando herramienta MCP...
üîß Herramienta: obtener_temperatura
üìù Argumentos: {'ciudad': 'Madrid', 'dias': 3}

ü§ñ Asistente: Seg√∫n el pron√≥stico para Madrid:

Ma√±ana (S√°bado 06/12/2025):
- Temperatura: Entre 9.8¬∞C y 19.1¬∞C
- Clima: Nublado
- Probabilidad de lluvia: 8%
- Viento: 12.5 km/h
```

## Ventajas de MCP sobre Tools Directas

1. **Separaci√≥n de responsabilidades**: El servidor MCP puede ejecutarse en otra m√°quina
2. **Reutilizaci√≥n**: M√∫ltiples clientes pueden conectarse al mismo servidor
3. **Est√°ndares**: MCP es un protocolo est√°ndar compatible con m√∫ltiples LLMs
4. **Escalabilidad**: F√°cil de escalar y distribuir

## Comparaci√≥n con Lecci√≥n 1

| Aspecto | Lecci√≥n 1 (Tools) | Lecci√≥n 2 (MCP) |
|---------|-------------------|-----------------|
| Arquitectura | Monol√≠tica | Cliente-Servidor |
| Reutilizaci√≥n | Baja | Alta |
| Complejidad | Simple | Moderada |
| Escalabilidad | Limitada | Excelente |
| Est√°ndar | Espec√≠fico | Protocolo MCP |

## Pr√≥ximos Pasos

- Crear servidores MCP con m√∫ltiples herramientas
- Conectar con APIs externas reales
- Implementar autenticaci√≥n en servidores MCP
- Desplegar servidores MCP en la nube

---

**Recursos**:
- [Documentaci√≥n MCP](https://modelcontextprotocol.io/)
- [MCP GitHub](https://github.com/modelcontextprotocol)
- [Ollama Docs](https://ollama.ai/docs)
