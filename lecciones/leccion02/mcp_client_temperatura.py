#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Cliente MCP simple que se conecta al servidor de temperatura
Integra Ollama para procesamiento de lenguaje natural
"""
import asyncio
import ollama
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

async def main():
    print("="*60)
    print("ğŸŒ CLIENTE MCP - Servidor de Temperatura")
    print("="*60)
    print("Este cliente se conecta a un servidor MCP que proporciona")
    print("informaciÃ³n meteorolÃ³gica de ciudades espaÃ±olas.\n")
    
    # ParÃ¡metros para iniciar el servidor MCP
    server_params = StdioServerParameters(
        command="python3",
        args=["mcp_server_temperatura.py"],
        env=None
    )
    
    try:
        # Conectar al servidor MCP
        print("ğŸ”Œ Conectando al servidor MCP...")
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                # Inicializar sesiÃ³n
                await session.initialize()
                
                # Listar herramientas disponibles
                tools = await session.list_tools()
                print(f"âœ… Conectado al servidor MCP")
                print(f"ğŸ“‹ Herramientas disponibles: {len(tools.tools)}\n")
                
                for tool in tools.tools:
                    print(f"   ğŸ”§ {tool.name}")
                    print(f"      â””â”€ {tool.description}\n")
                
                print("="*60)
                print("ğŸ’¬ Chat con el asistente meteorolÃ³gico")
                print("Escribe 'salir' para terminar\n")
                
                # Historial de mensajes
                mensajes = [
                    {
                        'role': 'system',
                        'content': 'Eres un asistente meteorolÃ³gico. Usa las herramientas disponibles para responder preguntas sobre el tiempo en ciudades espaÃ±olas. SÃ© amable y conciso.'
                    }
                ]
                
                # Loop de chat
                while True:
                    try:
                        pregunta = input("ğŸ‘¤ TÃº: ").strip()
                        
                        if pregunta.lower() in ['salir', 'exit', 'quit']:
                            print("\nğŸ‘‹ Â¡Hasta luego!")
                            break
                        
                        if not pregunta:
                            continue
                        
                        mensajes.append({'role': 'user', 'content': pregunta})
                        
                        # Convertir herramientas MCP a formato Ollama
                        tools_ollama = []
                        for tool in tools.tools:
                            tools_ollama.append({
                                'type': 'function',
                                'function': {
                                    'name': tool.name,
                                    'description': tool.description,
                                    'parameters': tool.inputSchema
                                }
                            })
                        
                        # Primera llamada: LLM decide si usar herramienta
                        print("ğŸ¤” Pensando...", end='', flush=True)
                        respuesta = ollama.chat(
                            model='llama3.2:3b',
                            messages=mensajes,
                            tools=tools_ollama
                        )
                        print("\r" + " "*50 + "\r", end='')  # Limpiar lÃ­nea
                        
                        # Â¿El LLM quiere usar una herramienta?
                        if respuesta['message'].get('tool_calls'):
                            print("âœ… Consultando servidor MCP...")
                            
                            tool_call = respuesta['message']['tool_calls'][0]
                            tool_name = tool_call['function']['name']
                            tool_args = tool_call['function']['arguments']
                            
                            print(f"   ğŸ”§ Herramienta: {tool_name}")
                            print(f"   ğŸ“ Ciudad: {tool_args.get('ciudad')}")
                            if 'dias' in tool_args:
                                print(f"   ğŸ“… DÃ­as: {tool_args.get('dias')}")
                            
                            # Llamar a la herramienta en el servidor MCP
                            resultado = await session.call_tool(tool_name, tool_args)
                            
                            resultado_texto = resultado.content[0].text
                            
                            # Debug: mostrar si hay error
                            if "Error" in resultado_texto or "error" in resultado_texto:
                                print(f"\nâš ï¸  DEBUG - Respuesta del servidor:")
                                print("   " + "\n   ".join(resultado_texto.split('\n')[:10]))
                            
                            # AÃ±adir resultado al historial
                            mensajes.append(respuesta['message'])
                            mensajes.append({
                                'role': 'tool',
                                'content': resultado_texto
                            })
                            
                            # Segunda llamada: LLM procesa el resultado
                            print("ğŸ§  Procesando informaciÃ³n...", end='', flush=True)
                            respuesta = ollama.chat(
                                model='llama3.2:3b',
                                messages=mensajes
                            )
                            print("\r" + " "*50 + "\r", end='')  # Limpiar lÃ­nea
                        
                        # Mostrar respuesta
                        print(f"\nğŸ¤– Asistente: {respuesta['message']['content']}\n")
                        mensajes.append(respuesta['message'])
                        
                    except KeyboardInterrupt:
                        print("\n\nğŸ‘‹ Interrumpido por el usuario. Â¡Hasta luego!")
                        break
                    except Exception as e:
                        print(f"\nâŒ Error: {str(e)}\n")
                        continue
                        
    except Exception as e:
        print(f"\nâŒ Error al conectar con el servidor MCP: {str(e)}")
        print("\nAsegÃºrate de que:")
        print("1. Tienes instalado el SDK de MCP: pip install mcp")
        print("2. Tienes instalado ollama: pip install ollama")
        print("3. El servidor Ollama estÃ¡ corriendo (docker o local)")
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
