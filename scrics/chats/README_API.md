# API Ollama Server

Servidor Flask que proporciona una API REST para interactuar con Ollama usando contextos de personajes y historial de conversaciones.

## ğŸš€ Inicio RÃ¡pido

### 1. Ejecutar el contenedor Docker
```bash
docker run --rm -d --name ollama \
  -p 11434:11434 -p 5000:5000 \
  --memory="16g" \
  -v /Users/T054810/ollama_local/scrics:/scrics \
  -v /Users/T054810/ia/ollama/usr_local_lib:/usr/local/lib \
  -v /Users/T054810/ia/ollama/root_ollama:/root/.ollama \
  -ti ollama
```

### 2. Ejecutar la API
```bash
docker exec -it ollama bash
cd /app/scrics/chats
python api_ollama_server.py
```

### 3. Probar la API
```bash
# Desde otra terminal
curl -X POST http://localhost:5000/preguntar \
     -H "Content-Type: application/json" \
     -d '{"persona":"jandro", "pregunta":"Â¿Como se llama mi padre"}'
```

## ğŸ“ Estructura de Archivos

```
/scrics/chats/
â”œâ”€â”€ api_ollama_server.py    # Servidor Flask principal
â”œâ”€â”€ test_api.sh             # Script de prueba
â”œâ”€â”€ historial/              # Historiales de conversaciÃ³n
â”‚   â””â”€â”€ {persona}_history.txt
â””â”€â”€ contextos/              # Contextos de personajes
    â””â”€â”€ {persona}.json
```

## ğŸ”§ Endpoints

### POST /preguntar
Hace una pregunta a un personaje especÃ­fico usando Ollama.

**ParÃ¡metros:**
- `persona` (string): Nombre del personaje
- `pregunta` (string): Pregunta a hacer

**Ejemplo:**
```bash
curl -X POST http://localhost:5000/preguntar \
     -H "Content-Type: application/json" \
     -d '{"persona":"jandro", "pregunta":"Â¿QuÃ© tiempo hace?"}'
```

**Respuesta exitosa:**
```json
{
  "persona": "jandro",
  "pregunta": "Â¿QuÃ© tiempo hace?",
  "respuesta": "Hace un dÃ­a soleado..."
}
```

### POST /resumir
Resume el historial de conversaciÃ³n de un personaje.

**ParÃ¡metros:**
- `persona` (string): Nombre del personaje

**Ejemplo:**
```bash
curl -X POST http://localhost:5000/resumir \
     -H "Content-Type: application/json" \
     -d '{"persona":"jandro"}'
```

## ğŸ“‹ Formato de Contextos

Los archivos de contexto deben estar en `/scrics/chats/contextos/{persona}.json`:

```json
{
  "nombre": "Jandro",
  "relacion": "amigo cercano",
  "personalidad": "amigable, sarcÃ¡stico, le gusta la tecnologÃ­a",
  "proyectos": [
    "Desarrollo de APIs",
    "Machine Learning",
    "Desarrollo web"
  ]
}
```

## ğŸ” SoluciÃ³n de Problemas

### Error 404 en Ollama
- Verificar que Ollama estÃ© corriendo: `curl http://localhost:11434/api/tags`
- Verificar que el modelo estÃ© disponible: `ollama list`

### Error de conexiÃ³n
- Asegurarse de que los puertos 11434 y 5000 estÃ©n mapeados
- Verificar que el contenedor estÃ© corriendo: `docker ps`

### Modelo no encontrado
- Cambiar `MODEL_NAME` en el cÃ³digo por un modelo disponible
- O instalar el modelo: `ollama pull llama3.1:8b`

## ğŸ§ª Pruebas

Ejecutar el script de prueba:
```bash
./test_api.sh
```

Esto verificarÃ¡ que:
- El contenedor estÃ© corriendo
- La API responda correctamente
- Ollama estÃ© disponible