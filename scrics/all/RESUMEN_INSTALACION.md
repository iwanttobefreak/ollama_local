# üöÄ RESUMEN EJECUTIVO - Tool de Temperatura para Ollama Remoto

## üìå LO QUE NECESITAS HACER (3 PASOS SIMPLES)

### 1Ô∏è‚É£ COPIAR EL ARCHIVO AL SERVIDOR

**Archivo a copiar:** `ollama_tool_standalone.py`  
**Ubicaci√≥n actual:** `C:\Users\joseantonio.legidoma\copilot\apis\ollama_tool_standalone.py`

**Opci√≥n r√°pida (SCP desde PowerShell):**
```powershell
scp C:\Users\joseantonio.legidoma\copilot\apis\ollama_tool_standalone.py usuario@tu-servidor:/home/usuario/
```

---

### 2Ô∏è‚É£ INSTALAR EN EL SERVIDOR

Con√©ctate al servidor por SSH y ejecuta:

```bash
# Instalar paquetes de Python
pip3 install ollama requests

# Descargar modelo (si no lo tienes)
ollama pull llama3.1
```

---

### 3Ô∏è‚É£ EJECUTAR LA TOOL

```bash
# Ejecutar chat interactivo
python3 ollama_tool_standalone.py
```

**¬°Y LISTO!** Ya puedes chatear sobre el tiempo.

---

## üí¨ EJEMPLO DE USO

```
Tu: ¬øQu√© tiempo har√° ma√±ana en Madrid?

[Consultando Madrid...]

Ollama: Ma√±ana en Madrid tendremos temperaturas entre 13.9¬∞C y 23.8¬∞C, 
con cielo nublado y 18% de probabilidad de lluvia.

Tu: ¬øY en Barcelona?

[Consultando Barcelona...]

Ollama: En Barcelona las temperaturas ser√°n de 17.5¬∞C a 21.3¬∞C...
```

---

## üìã COMANDO COMPLETO DE INSTALACI√ìN (COPIAR Y PEGAR)

En el servidor, ejecuta todo esto de una vez:

```bash
# Instalar dependencias
pip3 install ollama requests && \

# Verificar que Ollama est√° corriendo (si no, iniciarlo)
pgrep ollama || (echo "Iniciando Ollama..." && nohup ollama serve > /dev/null 2>&1 &) && \

# Descargar modelo si no existe
ollama list | grep -q llama3.1 || ollama pull llama3.1 && \

# Ejecutar la tool
python3 ollama_tool_standalone.py
```

---

## ‚úÖ VERIFICACI√ìN R√ÅPIDA

```bash
# Todo en uno
python3 -c "import ollama, requests; print('Paquetes OK')" && \
ollama list | grep llama3.1 && \
echo "Todo listo para ejecutar!"
```

---

## üéØ CARACTER√çSTICAS DE ESTA TOOL

- ‚úÖ **UN SOLO ARCHIVO** - Todo incluido, no necesita m√°s archivos
- ‚úÖ **CUALQUIER CIUDAD** - Madrid, Barcelona, Matar√≥, pueblos peque√±os, etc.
- ‚úÖ **SIN DATOS HARDCODEADOS** - B√∫squeda din√°mica
- ‚úÖ **SIN API_KEY** - Usa servicios gratuitos
- ‚úÖ **CHAT NATURAL** - Habla normalmente con Ollama
- ‚úÖ **16 D√çAS DE PRON√ìSTICO** - Todo el que necesites

---

## üìÅ ARCHIVO √öNICO

**ollama_tool_standalone.py**
- Tama√±o: ~8 KB
- Incluye: Todo lo necesario
- Dependencias externas: ollama, requests
- Funciona: De forma independiente

---

## üö® SI TIENES PROBLEMAS

### Ollama no responde
```bash
ollama serve
```

### Modelo no encontrado
```bash
ollama pull llama3.1
```

### Paquetes no instalados
```bash
pip3 install ollama requests
```

---

## üìû SOPORTE R√ÅPIDO

| Problema | Soluci√≥n |
|----------|----------|
| No encuentra el m√≥dulo ollama | `pip3 install ollama` |
| Connection refused | `ollama serve` |
| Model not found | `ollama pull llama3.1` |
| No funciona en Python 2 | Usa `python3` |

---

## üéì PREGUNTAS QUE PUEDES HACER

- "¬øQu√© tiempo har√° ma√±ana en Madrid?"
- "Pron√≥stico de 5 d√≠as para Barcelona"
- "¬øLlover√° en Sevilla esta semana?"
- "¬øD√≥nde har√° mejor tiempo: Madrid o Barcelona?"
- "Temperatura en Matar√≥ para el fin de semana"

---

## ‚è±Ô∏è TIEMPO ESTIMADO

- **Copiar archivo:** 1 minuto
- **Instalar dependencias:** 2 minutos
- **Descargar modelo (si no lo tienes):** 5-10 minutos
- **Primera prueba:** 1 minuto

**TOTAL:** ~5-15 minutos

---

## üéâ RESULTADO FINAL

Tendr√°s un chat con Ollama que puede responder preguntas sobre el tiempo en cualquier ciudad de Espa√±a, usando datos en tiempo real.

**Simple. R√°pido. Funciona.**

---

## üìù CHECKLIST

```
[ ] Archivo ollama_tool_standalone.py copiado al servidor
[ ] pip3 install ollama requests ejecutado
[ ] ollama serve corriendo
[ ] ollama pull llama3.1 completado
[ ] python3 ollama_tool_standalone.py ejecutado
[ ] Primera pregunta realizada
```

---

**Archivo a copiar:** `ollama_tool_standalone.py`  
**Comando para ejecutar:** `python3 ollama_tool_standalone.py`  
**Estado:** ‚úÖ Listo para usar
