# üöÄ Gu√≠a de Instalaci√≥n en Servidor Ollama Remoto

## üìã PASOS PARA INSTALAR LA TOOL EN TU SERVIDOR OLLAMA

### üì¶ Paso 1: Copiar el archivo al servidor

#### Opci√≥n A: Usando SCP (desde Windows)

```powershell
# Desde tu PC Windows
scp C:\Users\joseantonio.legidoma\copilot\apis\ollama_tool_standalone.py usuario@servidor:/home/usuario/
```

#### Opci√≥n B: Usando WinSCP o FileZilla

1. Abre WinSCP o FileZilla
2. Conecta a tu servidor
3. Navega a: `C:\Users\joseantonio.legidoma\copilot\apis\`
4. Copia `ollama_tool_standalone.py` al servidor
5. Col√≥calo en `/home/usuario/` o donde prefieras

#### Opci√≥n C: Copiar y pegar manualmente

1. Abre el archivo `ollama_tool_standalone.py` en Windows
2. Copia todo el contenido (Ctrl+A, Ctrl+C)
3. Con√©ctate al servidor por SSH
4. Crea el archivo:
   ```bash
   nano ollama_tool_standalone.py
   ```
5. Pega el contenido (Ctrl+Shift+V en la mayor√≠a de terminales)
6. Guarda (Ctrl+O, Enter, Ctrl+X)

---

### üîß Paso 2: Instalar dependencias en el servidor

Con√©ctate al servidor por SSH y ejecuta:

```bash
# Instalar paquetes de Python
pip install ollama requests

# O si usas pip3
pip3 install ollama requests
```

---

### ‚úÖ Paso 3: Verificar que Ollama est√° corriendo

En el servidor, verifica:

```bash
# Ver si Ollama est√° corriendo
ps aux | grep ollama

# O intentar conectar
curl http://localhost:11434/api/version
```

Si no est√° corriendo:

```bash
# Iniciar Ollama
ollama serve

# O en segundo plano
nohup ollama serve > /dev/null 2>&1 &
```

---

### üì• Paso 4: Descargar modelo (si no lo tienes)

```bash
# Verificar modelos instalados
ollama list

# Si no tienes llama3.1, descargarlo
ollama pull llama3.1
```

---

### üöÄ Paso 5: Ejecutar la tool

```bash
# Chat interactivo
python3 ollama_tool_standalone.py

# O modo test
python3 ollama_tool_standalone.py --test
```

---

## üí¨ Ejemplo de Uso

```bash
$ python3 ollama_tool_standalone.py
======================================================================
CHAT DE TEMPERATURA CON OLLAMA
======================================================================

Ejemplos de preguntas:
  - ¬øQue tiempo hara ma√±ana en Madrid?
  - Pronostico de 5 dias para Barcelona
  - ¬øLlovera en Sevilla esta semana?

Escribe 'salir' para terminar

Tu: ¬øQu√© tiempo har√° ma√±ana en Madrid?

[Consultando Madrid...]

Ollama: Seg√∫n el pron√≥stico, ma√±ana en Madrid tendremos temperaturas 
entre 13.9¬∞C y 23.8¬∞C, con cielo nublado y 18% de probabilidad de 
lluvia. No ser√° necesario paraguas.

Tu: ¬øY en Barcelona?

[Consultando Barcelona...]

Ollama: En Barcelona las temperaturas estar√°n entre 17.5¬∞C y 21.3¬∞C,
parcialmente nublado con 15% de probabilidad de lluvia.

Tu: salir
Adios!
```

---

## üîç Verificar Instalaci√≥n

### Test completo:

```bash
# 1. Verificar Python
python3 --version

# 2. Verificar paquetes
python3 -c "import ollama; import requests; print('OK')"

# 3. Verificar Ollama
curl http://localhost:11434/api/version

# 4. Verificar modelo
ollama list | grep llama3.1

# 5. Probar la tool
python3 ollama_tool_standalone.py --test
```

---

## üìù Script de Instalaci√≥n Autom√°tica

Guarda esto como `instalar_tool.sh` en el servidor:

```bash
#!/bin/bash

echo "=================================================="
echo "Instalando Tool de Temperatura para Ollama"
echo "=================================================="
echo ""

# 1. Verificar Python
echo "[1/5] Verificando Python..."
if command -v python3 &> /dev/null; then
    echo "‚úì Python3 encontrado: $(python3 --version)"
else
    echo "‚úó Python3 no encontrado. Instalalo primero."
    exit 1
fi

# 2. Instalar dependencias
echo ""
echo "[2/5] Instalando dependencias..."
pip3 install ollama requests
if [ $? -eq 0 ]; then
    echo "‚úì Dependencias instaladas"
else
    echo "‚úó Error al instalar dependencias"
    exit 1
fi

# 3. Verificar Ollama
echo ""
echo "[3/5] Verificando Ollama..."
if pgrep -x "ollama" > /dev/null; then
    echo "‚úì Ollama est√° corriendo"
else
    echo "‚ö† Ollama no est√° corriendo"
    echo "  Ejecuta: ollama serve"
fi

# 4. Verificar modelo
echo ""
echo "[4/5] Verificando modelo llama3.1..."
if ollama list | grep -q "llama3.1"; then
    echo "‚úì Modelo llama3.1 encontrado"
else
    echo "‚ö† Modelo llama3.1 no encontrado"
    echo "  Descargando modelo (esto puede tardar)..."
    ollama pull llama3.1
fi

# 5. Probar la tool
echo ""
echo "[5/5] Probando la tool..."
if [ -f "ollama_tool_standalone.py" ]; then
    python3 ollama_tool_standalone.py --test
    echo ""
    echo "=================================================="
    echo "‚úì Instalaci√≥n completada"
    echo "=================================================="
    echo ""
    echo "Para usar, ejecuta:"
    echo "  python3 ollama_tool_standalone.py"
else
    echo "‚úó Archivo ollama_tool_standalone.py no encontrado"
    echo "  C√≥pialo al servidor primero"
fi
```

Luego ejecuta:

```bash
chmod +x instalar_tool.sh
./instalar_tool.sh
```

---

## üêõ Soluci√≥n de Problemas

### Error: "ModuleNotFoundError: No module named 'ollama'"

```bash
pip3 install ollama
# o
python3 -m pip install ollama
```

### Error: "Connection refused"

Ollama no est√° corriendo:
```bash
# Iniciar Ollama
ollama serve

# En segundo plano
nohup ollama serve > /dev/null 2>&1 &
```

### Error: "Model not found"

```bash
ollama pull llama3.1
```

### La tool no llama a la funci√≥n

Verifica que el modelo soporta function calling:
```bash
# Usa llama3.1 (recomendado)
ollama pull llama3.1
```

---

## üîê Permisos

Si tienes problemas de permisos:

```bash
# Dar permisos de ejecuci√≥n
chmod +x ollama_tool_standalone.py

# Ejecutar
./ollama_tool_standalone.py
```

---

## üåê Configuraci√≥n para Diferentes Modelos

Si quieres usar otro modelo, edita el archivo:

```python
# L√≠nea ~235 aproximadamente
response = ollama.chat(
    model='llama3.1',  # <-- Cambia aqu√≠
    messages=messages,
    tools=[TOOL_DEFINITION]
)
```

Modelos compatibles:
- `llama3.1` ‚úÖ Recomendado
- `llama3.2` ‚úÖ
- `mistral` ‚úÖ
- `mixtral` ‚úÖ
- `qwen2.5` ‚úÖ

---

## üìä Resumen de Comandos

### En tu PC Windows:
```powershell
# Copiar archivo al servidor
scp ollama_tool_standalone.py usuario@servidor:/home/usuario/
```

### En el servidor Linux:
```bash
# Instalar dependencias
pip3 install ollama requests

# Verificar Ollama
ollama serve

# Descargar modelo
ollama pull llama3.1

# Ejecutar tool
python3 ollama_tool_standalone.py
```

---

## ‚úÖ Checklist Final

- [ ] Archivo `ollama_tool_standalone.py` copiado al servidor
- [ ] Dependencias instaladas (`pip3 install ollama requests`)
- [ ] Ollama corriendo (`ollama serve`)
- [ ] Modelo descargado (`ollama pull llama3.1`)
- [ ] Tool ejecut√°ndose (`python3 ollama_tool_standalone.py`)
- [ ] Primera pregunta realizada y respondida

---

## üéâ ¬°Listo!

Una vez completados todos los pasos, tendr√°s tu asistente de temperatura funcionando en el servidor Ollama.

**Archivo a copiar:** `ollama_tool_standalone.py`
**Tama√±o:** ~8 KB
**Dependencias:** ollama, requests
**Tiempo de instalaci√≥n:** ~5 minutos

---

## üìû Preguntas Frecuentes

**P: ¬øNecesito conexi√≥n a internet en el servidor?**
R: S√≠, para descargar el modelo de Ollama y para que la tool consulte las APIs de clima.

**P: ¬øPuedo usar otro modelo que no sea llama3.1?**
R: S√≠, cualquier modelo que soporte function calling (llama3.2, mistral, mixtral, etc.)

**P: ¬øFunciona en Windows Server?**
R: S√≠, los comandos son similares pero usa `python` en lugar de `python3`.

**P: ¬øCu√°nta RAM necesita?**
R: Depende del modelo de Ollama (llama3.1 necesita ~4-8GB).

**P: ¬øPuedo tener m√∫ltiples usuarios usando la tool?**
R: S√≠, cada usuario puede ejecutar su propia instancia del script.

---

**√öltima actualizaci√≥n:** 15/10/2025
**Estado:** ‚úÖ Probado y funcionando
