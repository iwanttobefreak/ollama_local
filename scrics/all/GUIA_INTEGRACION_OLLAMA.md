# GuÃ­a de IntegraciÃ³n con Ollama

## ğŸš€ IntegraciÃ³n RÃ¡pida (3 pasos)

### Paso 1: Verificar que Ollama estÃ¡ corriendo

Ya tienes `ollama serve` corriendo âœ…

Verifica que tienes un modelo instalado:
```powershell
ollama list
```

Si no tienes llama3.1, instÃ¡lalo:
```powershell
ollama pull llama3.1
```

### Paso 2: Instalar el paquete de Python de Ollama

```powershell
pip install ollama
```

### Paso 3: Ejecutar la tool

```powershell
cd C:\Users\joseantonio.legidoma\copilot\apis
python ollama_temperatura_tool.py
```

Â¡Y listo! Ya puedes chatear con Ollama sobre el tiempo.

---

## ğŸ’¬ Ejemplo de ConversaciÃ³n

```
======================================================================
CHAT DE TEMPERATURA CON OLLAMA
Pregunta sobre el tiempo en CUALQUIER ciudad de EspaÃ±a
======================================================================

Ejemplos de preguntas:
  - Â¿Que tiempo hara maÃ±ana en Madrid?
  - Pronostico de 5 dias para Mataro
  - Â¿Llovera en Alcobendas esta semana?
  - Temperatura en Barcelona los proximos 3 dias

Escribe 'salir' para terminar

Tu: Â¿QuÃ© tiempo harÃ¡ maÃ±ana en Madrid?

[Consultando Madrid...]

Ollama: SegÃºn el pronÃ³stico, maÃ±ana (jueves 16/10) en Madrid tendremos:
- Temperaturas entre 13.9Â°C y 23.8Â°C
- Cielo nublado
- 18% de probabilidad de lluvia
- Viento moderado de 7.2 km/h

SerÃ¡ un dÃ­a agradable, con temperaturas cÃ¡lidas. No parece necesario paraguas.

Tu: Â¿Y en Barcelona?

[Consultando Barcelona...]

Ollama: En Barcelona para maÃ±ana el pronÃ³stico es:
- Temperaturas entre 17.5Â°C y 21.3Â°C
- Parcialmente nublado
- 15% de probabilidad de lluvia
- Viento de 11 km/h

Algo mÃ¡s fresco que en Madrid, pero tambiÃ©n buen tiempo.

Tu: salir
Adios!
```

---

## ğŸ”§ ConfiguraciÃ³n Avanzada

### OpciÃ³n 1: Chat Interactivo (Recomendado)

```powershell
python ollama_temperatura_tool.py
```

Ventajas:
- âœ… ConversaciÃ³n natural
- âœ… MÃºltiples preguntas seguidas
- âœ… Ollama procesa y resume los datos
- âœ… Contexto mantenido

### OpciÃ³n 2: Usar en tu Propio Script

Crea un archivo `mi_chat_tiempo.py`:

```python
from ollama_temperatura_tool import obtener_pronostico_temperatura, TOOL_DEFINITION
import ollama

# Funciones disponibles
available_functions = {
    'obtener_pronostico_temperatura': obtener_pronostico_temperatura
}

def preguntar(pregunta):
    """Hace una pregunta sobre el tiempo"""
    
    # Llamar a Ollama
    response = ollama.chat(
        model='llama3.1',
        messages=[
            {'role': 'user', 'content': pregunta}
        ],
        tools=[TOOL_DEFINITION]
    )
    
    # Si Ollama quiere usar la herramienta
    if response['message'].get('tool_calls'):
        messages = [
            {'role': 'user', 'content': pregunta},
            response['message']
        ]
        
        for tool_call in response['message']['tool_calls']:
            function_name = tool_call['function']['name']
            function_args = tool_call['function']['arguments']
            
            # Ejecutar funciÃ³n
            if function_name in available_functions:
                print(f"[Consultando {function_args.get('ciudad')}...]")
                function_response = available_functions[function_name](**function_args)
                
                messages.append({
                    'role': 'tool',
                    'content': function_response
                })
        
        # Obtener respuesta final
        final_response = ollama.chat(
            model='llama3.1',
            messages=messages
        )
        
        return final_response['message']['content']
    else:
        return response['message']['content']

# Usar
respuesta = preguntar("Â¿QuÃ© tiempo harÃ¡ en Madrid los prÃ³ximos 3 dÃ­as?")
print(respuesta)
```

### OpciÃ³n 3: FunciÃ³n Directa (Sin Chat)

Si solo quieres los datos sin que Ollama los procese:

```python
from ollama_temperatura_tool import obtener_pronostico_temperatura

# Obtener pronÃ³stico directamente
resultado = obtener_pronostico_temperatura("Madrid", 3)
print(resultado)
```

---

## ğŸ¯ Modelos de Ollama Compatibles

La tool funciona con cualquier modelo que soporte function calling:

- âœ… **llama3.1** (Recomendado)
- âœ… **llama3.2**
- âœ… **mistral**
- âœ… **mixtral**
- âœ… **qwen2.5**

Para cambiar de modelo, edita la lÃ­nea en `ollama_temperatura_tool.py`:
```python
response = ollama.chat(
    model='llama3.1',  # Cambia aquÃ­ el modelo
    messages=messages,
    tools=[TOOL_DEFINITION]
)
```

---

## ğŸ› SoluciÃ³n de Problemas

### Error: "Import ollama could not be resolved"

```powershell
pip install ollama
```

### Error: "Connection refused"

Verifica que Ollama estÃ¡ corriendo:
```powershell
# En otra terminal
ollama serve
```

O verifica el proceso:
```powershell
Get-Process ollama
```

### Error: "Model not found"

Descarga el modelo:
```powershell
ollama pull llama3.1
```

### Ollama no usa la herramienta

Algunas preguntas que funcionan mejor:
- âœ… "Â¿QuÃ© tiempo harÃ¡ en Madrid?"
- âœ… "PronÃ³stico para Barcelona"
- âœ… "Â¿LloverÃ¡ maÃ±ana en Sevilla?"

Evita:
- âŒ "Hola" (muy genÃ©rico)
- âŒ "Â¿CÃ³mo estÃ¡s?" (no relacionado con tiempo)

---

## ğŸ“Š Ejemplo Paso a Paso

### 1. Instalar Ollama (si no lo tienes)
```powershell
# Descargar de https://ollama.ai
# Ejecutar instalador
ollama serve
```

### 2. Instalar modelo
```powershell
ollama pull llama3.1
```

### 3. Instalar paquete Python
```powershell
pip install ollama
```

### 4. Ejecutar la tool
```powershell
cd C:\Users\joseantonio.legidoma\copilot\apis
python ollama_temperatura_tool.py
```

### 5. Hacer preguntas
```
Tu: Â¿QuÃ© tiempo harÃ¡ maÃ±ana en Madrid?
Tu: Â¿Y pasado maÃ±ana?
Tu: Â¿DÃ³nde harÃ¡ mejor tiempo: Madrid o Barcelona?
Tu: salir
```

---

## ğŸ“ Ejemplos de Preguntas

### Preguntas Simples
- "Â¿QuÃ© tiempo harÃ¡ en Madrid?"
- "PronÃ³stico para Barcelona"
- "Temperatura en Sevilla"

### Con Detalles Temporales
- "Â¿QuÃ© tiempo harÃ¡ maÃ±ana en Valencia?"
- "PronÃ³stico de 5 dÃ­as para MÃ¡laga"
- "Â¿CÃ³mo estarÃ¡ el tiempo este fin de semana en Bilbao?"

### Preguntas EspecÃ­ficas
- "Â¿LloverÃ¡ maÃ±ana en Murcia?"
- "Â¿Necesito abrigo en Madrid esta semana?"
- "Â¿HarÃ¡ calor en Sevilla los prÃ³ximos dÃ­as?"

### Comparaciones
- "Â¿DÃ³nde harÃ¡ mejor tiempo: Madrid o Barcelona?"
- "Compara el tiempo de Valencia y Alicante"

### Cualquier Ciudad
- "Tiempo en MatarÃ³"
- "Â¿QuÃ© tiempo harÃ¡ en Alcobendas?"
- "PronÃ³stico para San SebastiÃ¡n"

---

## ğŸ’¡ Tips de Uso

### 1. Preguntas Naturales
Ollama entiende lenguaje natural. No necesitas comandos especÃ­ficos.

### 2. Contexto Mantenido
Puedes hacer preguntas de seguimiento:
```
Tu: Â¿QuÃ© tiempo harÃ¡ en Madrid?
Ollama: [responde]
Tu: Â¿Y en Barcelona?  <- Entiende que sigues preguntando por el tiempo
```

### 3. MÃºltiples Ciudades
Ollama puede comparar automÃ¡ticamente:
```
Tu: Â¿DÃ³nde lloverÃ¡ mÃ¡s: Madrid o Barcelona?
```
Ollama consultarÃ¡ ambas ciudades y compararÃ¡.

### 4. InterpretaciÃ³n Inteligente
Ollama interpreta y resume los datos:
```
Tu: Â¿Necesito paraguas maÃ±ana en Madrid?
Ollama: ConsultarÃ¡ el pronÃ³stico y responderÃ¡ basÃ¡ndose en la probabilidad de lluvia
```

---

## ğŸ”„ IntegraciÃ³n con Otras Herramientas

Puedes combinar esta tool con otras tools de Ollama:

```python
from ollama_temperatura_tool import obtener_pronostico_temperatura, TOOL_DEFINITION as TEMP_TOOL
from ollama_ine import obtener_poblacion_ine, TOOL_DEFINITION as INE_TOOL
import ollama

# Combinar mÃºltiples herramientas
response = ollama.chat(
    model='llama3.1',
    messages=[
        {'role': 'user', 'content': 'Â¿CuÃ¡ntos habitantes tiene Madrid y quÃ© tiempo harÃ¡ maÃ±ana?'}
    ],
    tools=[TEMP_TOOL, INE_TOOL]
)
```

---

## âœ… Checklist de IntegraciÃ³n

- [ ] Ollama instalado
- [ ] `ollama serve` corriendo
- [ ] Modelo descargado (`ollama pull llama3.1`)
- [ ] Paquete Python instalado (`pip install ollama`)
- [ ] Tool descargada (`ollama_temperatura_tool.py`)
- [ ] Script ejecutÃ¡ndose (`python ollama_temperatura_tool.py`)
- [ ] Primera pregunta realizada

---

## ğŸ‰ Â¡Listo para Usar!

Una vez completados los pasos, simplemente ejecuta:

```powershell
python ollama_temperatura_tool.py
```

Y empieza a preguntar sobre el tiempo en cualquier ciudad de EspaÃ±a.

**Â¡Disfruta de tu asistente meteorolÃ³gico con IA!** â˜ï¸ğŸŒ¤ï¸â›ˆï¸
